{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 评价函数 Metric used for this competition \n",
    "# (Quadratic Weigthed Kappa aka Quadratic Cohen Kappa Score)\n",
    "def metric(y1,y2):\n",
    "    return cohen_kappa_score(y1, y2, weights = 'quadratic')\n",
    "\n",
    "\n",
    "# Make scorer for scikit-learn\n",
    "scorer = make_scorer(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#\n",
    "def split_score(model, x, y, n=10):\n",
    "    y_pre = np.zeros(y.shape[0])\n",
    "    kfold = StratifiedKFold(n_splits=n, random_state=4)\n",
    "    for train_index, test_index in kfold.split(x,y):\n",
    "        model.fit(x.iloc[train_index], y.iloc[train_index])\n",
    "        y_pre[test_index] = model.predict(x.iloc[test_index])\n",
    "    \n",
    "#    score = metric(y_pre, y)\n",
    "    print(\"{}折后的Kappa加权得分为:带补充\".format(n))\n",
    "    \n",
    "    return y_pre\n",
    "\n",
    "#\n",
    "def fix_y(y, coef):\n",
    "    y_fix = np.copy(y)\n",
    "    for i, pred in enumerate(y_fix):\n",
    "        if pred < coef[0]:\n",
    "            y_fix[i] = 0\n",
    "        elif pred >= coef[0] and pred < coef[1]:\n",
    "            y_fix[i] = 1\n",
    "        elif pred >= coef[1] and pred < coef[2]:\n",
    "            y_fix[i] = 2\n",
    "        elif pred >= coef[2] and pred < coef[3]:\n",
    "            y_fix[i] = 3\n",
    "        else:\n",
    "            y_fix[i] = 4    \n",
    "    return y_fix\n",
    "\n",
    "# \n",
    "def _kappa_loss(y, y_true, coef):\n",
    "    y_fix = np.copy(y)\n",
    "    for i, pred in enumerate(y_fix):\n",
    "        if pred < coef[0]:\n",
    "            y_fix[i] = 0\n",
    "        elif pred >= coef[0] and pred < coef[1]:\n",
    "            y_fix[i] = 1\n",
    "        elif pred >= coef[1] and pred < coef[2]:\n",
    "            y_fix[i] = 2\n",
    "        elif pred >= coef[2] and pred < coef[3]:\n",
    "            y_fix[i] = 3\n",
    "        else:\n",
    "            y_fix[i] = 4\n",
    "            \n",
    "    loss = metric(y_fix, y_true)\n",
    "    return -loss\n",
    "\n",
    "# 寻找分类的最佳参数\n",
    "def search_coef(x1, x2):\n",
    "    loss_partial = partial(_kappa_loss, x1, x2)\n",
    "    initial_coef = [1.55, 2.05, 2.5, 3]\n",
    "    coef = sp.optimize.basinhopping(loss_partial, initial_coef, niter=500, T=1,\n",
    "                                              stepsize=0.2, minimizer_kwargs={\"method\": 'nelder-mead'}, \n",
    "                                              take_step=None, accept_test=None, callback=None, \n",
    "                                              interval=100, disp=True, niter_success=10, seed=None)\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据、划分验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train  = pd.read_csv('train.csv')\n",
    "x = df_train.copy()\n",
    "\n",
    "#读取唯一的RescuerID\n",
    "RescuerID = set(df_train['RescuerID'].unique())\n",
    "\n",
    "#随机生成RescuerID\n",
    "j_test = random.sample(RescuerID, int(len(RescuerID)*0.2))\n",
    "j_train = RescuerID - set(j_test)\n",
    "\n",
    "df_test = df_train[df_train['RescuerID'].isin(j_test)]\n",
    "df_train = df_train[df_train['RescuerID'].isin(j_train)]\n",
    "\n",
    "train = df_train.copy()\n",
    "test  = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_breed = pd.read_csv('breed_labels.csv')\n",
    "labels_state = pd.read_csv('color_labels.csv')\n",
    "labels_color = pd.read_csv('state_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 删除异常值\n",
    "cul_drop = ['375905770', 'da8d4a273', '27e74e45c', '7b5bee232', '0327b8e94']\n",
    "df_train = df_train[~df_train['PetID'].isin(cul_drop)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取 sentiment 的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 12147 out of 12147 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done 2846 out of 2846 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "def extract_sentiment_feature(i, x):    \n",
    "#    feature_sentiment = pd.DataFrame(columns=['PetID', 'token', 'sentence_magnitude', 'sentence_score','document_magnitude', 'document_score'])\n",
    "    feature_sentiment = pd.DataFrame()\n",
    "\n",
    "    if x == 'train':\n",
    "        set_file = 'train'\n",
    "    else:\n",
    "        set_file = 'train' \n",
    "        \n",
    "    file_name = '{}_sentiment/{}.json'.format(set_file,i)\n",
    "    try:\n",
    "        f = open(file_name, 'r')\n",
    "        sentiment_file = json.load(f)\n",
    "            \n",
    "        token = [x['name'] for x in sentiment_file['entities']]\n",
    "        token = ' '.join(token)\n",
    "            \n",
    "        sentences_sentiment = [x['sentiment'] for x in sentiment_file['sentences']]\n",
    "        sentences_sentiment = pd.DataFrame.from_dict(\n",
    "            sentences_sentiment, orient='columns').sum()\n",
    "        sentenceSentiment_magnitude = sentences_sentiment['magnitude']\n",
    "        sentenceSentiment_score     = sentences_sentiment['score']\n",
    "            \n",
    "        docementSentiment_magnitude = sentiment_file['documentSentiment']['magnitude']\n",
    "        documentSentiment_score     = sentiment_file['documentSentiment']['score']\n",
    "            \n",
    "        new = pd.DataFrame(\n",
    "                {'PetID'               :[i], \n",
    "#                 'token'               : [token],\n",
    "                 'sentence_magnitude'  : [sentenceSentiment_magnitude],\n",
    "                 'sentence_score'      : [sentenceSentiment_score],\n",
    "                 'document_magnitude'  : [docementSentiment_magnitude], \n",
    "                 'document_score'      : [documentSentiment_score]})  \n",
    "        feature_sentiment = feature_sentiment.append(new)\n",
    "    except:\n",
    "        print('{}没找到'.format(file_name))\n",
    "    \n",
    "    for each in feature_sentiment.columns:\n",
    "        if each not in ['PetID','token']:\n",
    "            feature_sentiment[each] = feature_sentiment[each].astype(float)\n",
    "\n",
    "    return feature_sentiment\n",
    "\n",
    "#%%\n",
    "train_feature_sentiment = Parallel(n_jobs=8, verbose=1)(\n",
    "        delayed(extract_sentiment_feature)(i, 'train') for i in train.PetID)\n",
    "train_feature_sentiment = [x for x in train_feature_sentiment]\n",
    "train_feature_sentiment = pd.concat(train_feature_sentiment, ignore_index=True, sort=False)\n",
    "\n",
    "test_feature_sentiment = Parallel(n_jobs=8, verbose=1)(\n",
    "        delayed(extract_sentiment_feature)(i, 'test') for i in test.PetID)\n",
    "test_feature_sentiment = [x for x in test_feature_sentiment]\n",
    "test_feature_sentiment = pd.concat(test_feature_sentiment, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取 metadata 的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  52 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=8)]: Done 352 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=8)]: Done 852 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=8)]: Done 1552 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=8)]: Done 2452 tasks      | elapsed:   49.3s\n",
      "[Parallel(n_jobs=8)]: Done 3552 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=8)]: Done 4852 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=8)]: Done 6352 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=8)]: Done 8052 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=8)]: Done 9952 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=8)]: Done 11452 out of 11452 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  52 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=8)]: Done 352 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=8)]: Done 852 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=8)]: Done 1552 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=8)]: Done 2452 tasks      | elapsed:   50.1s\n",
      "[Parallel(n_jobs=8)]: Done 3526 out of 3541 | elapsed:  1.2min remaining:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 3541 out of 3541 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "#%% 提取 metadata 的特征\n",
    "#file_name = 'train_metadata/000a290e4-1.json'\n",
    "#f = open(file_name, 'r')\n",
    "#metadatafile = json.load(f)\n",
    "def extract_metadata_feature(i, x):\n",
    "    feature_metadata = pd.DataFrame()\n",
    "    if x == 'train':\n",
    "        set_file = 'train'\n",
    "    else:\n",
    "        set_file = 'train'\n",
    "        \n",
    "    metadata_filenames = sorted(glob.glob('{}_metadata/{}*.json'.format(set_file, i)))\n",
    "    if len(metadata_filenames) > 0:\n",
    "        feature_metadata_sub = pd.DataFrame(columns=['PetID', 'annots_score', 'color_score', 'color_pixelfrac', 'crop_conf','crop_importance', 'annots_top_desc'])\n",
    "        for ff in metadata_filenames:\n",
    "            f = open(ff, 'rb')\n",
    "            file = json.load(f)\n",
    "            #label\n",
    "            if 'labelAnnotations' in file:\n",
    "                file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n",
    "                file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "                file_top_desc = [x['description'] for x in file_annots]            \n",
    "            else:\n",
    "                file_top_score = np.nan\n",
    "                file_top_desc = ['']\n",
    "            #colors\n",
    "            file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']            \n",
    "            file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "            file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()            \n",
    "            #crops\n",
    "            file_crops = file['cropHintsAnnotation']['cropHints']                \n",
    "            file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "            if 'importanceFraction' in file_crops[0].keys():\n",
    "                file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "            else:\n",
    "                file_crop_importance = np.nan\n",
    "                \n",
    "            new = pd.DataFrame(\n",
    "                    {\n",
    "                            'PetID'          : [i],\n",
    "                            'annots_score'   : [file_top_score],\n",
    "                            'color_score'     : [file_color_score],\n",
    "                            'color_pixelfrac' : [file_color_pixelfrac],\n",
    "                            'crop_conf'       : [file_crop_conf],\n",
    "                            'crop_importance' : [file_crop_importance],\n",
    "                            'annots_top_desc' : [' '.join(file_top_desc)]})\n",
    "            feature_metadata_sub = feature_metadata_sub.append(new)\n",
    "                \n",
    "        metadata_desc = feature_metadata_sub.groupby(['PetID'])['annots_top_desc'].unique()\n",
    "        metadata_desc = metadata_desc.reset_index()\n",
    "        metadata_desc['annots_top_desc'] = metadata_desc['annots_top_desc'].apply(lambda x:' '.join(x))\n",
    "        feature_metadata_sub.drop(['annots_top_desc'], axis=1, inplace=True)\n",
    "\n",
    "        for each in feature_metadata_sub:\n",
    "            if each not in ['PetID']:\n",
    "                feature_metadata_sub[each] = feature_metadata_sub[each].astype(float)\n",
    "        \n",
    "        \n",
    "        feature_metadata_sub = feature_metadata_sub.groupby(['PetID']).agg(['mean', 'sum'])\n",
    "        feature_metadata_sub.columns = ['{}_{}'.format(c[0], c[1].upper()) for c in feature_metadata_sub.columns.tolist()]  \n",
    "        feature_metadata_sub = feature_metadata_sub.reset_index()\n",
    "        \n",
    "        feature_metadata_sub = feature_metadata_sub.merge(metadata_desc, how='left', on='PetID')\n",
    "            \n",
    "        feature_metadata = feature_metadata.append(feature_metadata_sub)\n",
    "    return feature_metadata\n",
    "\n",
    "#\n",
    "#for each in \n",
    "train_feature_metadata = Parallel(n_jobs=8, verbose=1)(\n",
    "        delayed(extract_metadata_feature)(i, 'train') for i in train.PetID)\n",
    "train_feature_metadata = [x for x in train_feature_metadata]\n",
    "train_feature_metadata = pd.concat(train_feature_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "test_feature_metadata = Parallel(n_jobs=8, verbose=1)(\n",
    "        delayed(extract_metadata_feature)(i, 'test') for i in test.PetID)\n",
    "test_feature_metadata = [x for x in test_feature_metadata]\n",
    "test_feature_metadata = pd.concat(test_feature_metadata, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cat black cat\n",
       "Name: annots_top_desc, dtype: object"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_metadata[train_feature_metadata['PetID']=='86e1089a3']['annots_top_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 连接sentiment和metadata和原始数据\n",
    "x_train = df_train.merge(train_feature_sentiment, how='left', on='PetID')\n",
    "#x_train = x_train.merge(train_feature_metadata, how='left', on='PetID')\n",
    "\n",
    "y_train = x_train['AdoptionSpeed']\n",
    "x_train.drop(['AdoptionSpeed'], axis=1, inplace=True)\n",
    "\n",
    "x_test = df_test.merge(test_feature_sentiment, how='left', on='PetID')\n",
    "#x_test = x_test.merge(test_feature_metadata, how='left', on='PetID')\n",
    "\n",
    "y_test = x_test['AdoptionSpeed']\n",
    "x_test.drop(['AdoptionSpeed'],  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP(情感分析，形容词的数量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "x = x_train.append(x_test)\n",
    "x = x[['PetID', 'Description']]\n",
    "\n",
    "x['Description'] = x['Description'].fillna(\"Missing\")\n",
    "x['Description'] = x['Description'].apply(lambda x:TextBlob(x))\n",
    "\n",
    "x['polarity']     = x['Description'].apply(lambda x:x.sentiment[0])\n",
    "x['subjectivity'] = x['Description'].apply(lambda x:x.sentiment[1])\n",
    "\n",
    "x_train = x_train.merge(x[['PetID', 'polarity', 'subjectivity']], how='left', on='PetID')\n",
    "x_Test  = x_test.merge(x[['PetID', 'polarity', 'subjectivity']], how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "##################################统计TF###############################################\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "x = x_train.append(x_test)\n",
    "x = x[['PetID', 'Description']]\n",
    "\n",
    "# 填充缺失值\n",
    "x['Description'] = x['Description'].fillna('Missing')\n",
    "\n",
    "# 第一步分词\n",
    "x['Description'] = x['Description'].apply(lambda x : nltk.word_tokenize(x))\n",
    "\n",
    "\n",
    "#1 去掉标点符号和停用词\n",
    "#去掉标点符号\n",
    "english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "x['Description'] = x['Description'].apply(lambda x : [word for word in x if word not in english_punctuations])\n",
    "\n",
    "# 提取布朗语料库的形容词\n",
    "brown_corpus = (nltk.corpus.brown.tagged_words())\n",
    "objective = [word for (word, tap) in brown_corpus if tap == 'JJ']\n",
    "# 根据布朗语料库提取形容词\n",
    "x['Description'] = x['Description'].apply(lambda x:list(set(x).intersection(set(objective))))\n",
    "x['Description'] = x['Description'].apply(lambda x:\" \".join(x))\n",
    "\n",
    "x['Adjective_count'] = x['Description'].apply(lambda x:len(x))\n",
    "#######构造词频统计\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#\n",
    "#countvec = CountVectorizer( lowercase  = True, \n",
    "#                            stop_words = 'english',\n",
    "#                            min_df     = 10,\n",
    "#                            max_df     = 0.9\n",
    "#                            )\n",
    "#b = countvec.fit_transform(x['Description'])\n",
    "\n",
    "##################################SVD降维###############################################\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "\n",
    "#countvec = CountVectorizer( lowercase  = True, \n",
    "#                            stop_words = 'english',\n",
    "#                            min_df     = 10,\n",
    "#                            max_df     = 0.9\n",
    "#                            )\n",
    "#TF_data = countvec.fit_transform(x['Description'])\n",
    "\n",
    "#n_components = 10\n",
    "#svd_ = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "#nmf_ = NMF(n_components=n_components)\n",
    "\n",
    "#nmf_col = nmf_.fit_transform(TF_data)\n",
    "#nmf_col = pd.DataFrame(nmf_col)\n",
    "#nmf_col = nmf_col.add_prefix('SVD_{}_'.format('describe'))\n",
    "\n",
    "#x = pd.concat([x.reset_index()['PetID'], nmf_col], axis=1)\n",
    "\n",
    "x_train = x_train.merge(x[['PetID', 'Adjective_count']], how='left', on='PetID')\n",
    "x_test  =  x_test.merge(x[['PetID', 'Adjective_count']], how='left', on='PetID')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Adjective_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6296e909a</td>\n",
       "      <td>alone near just temporary</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3422e4906</td>\n",
       "      <td>near adorable pregnant acceptable long precaut...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5842f1ff5</td>\n",
       "      <td>interested alert Good good very master active</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>850a43f90</td>\n",
       "      <td>such like looking cute playful young handsome</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d24c30b4b</td>\n",
       "      <td>stray</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1caa6fcdb</td>\n",
       "      <td>interested just outside still</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97aa9eeac</td>\n",
       "      <td>just</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c06d167ca</td>\n",
       "      <td>healthy active sure</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8b693ca84</td>\n",
       "      <td>serious</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aaedd873d</td>\n",
       "      <td>neuter happy active very</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4a9793dfb</td>\n",
       "      <td>long cute stray evident good</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c02be41e6</td>\n",
       "      <td>healthy red</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1fd342e17</td>\n",
       "      <td>fun-loving great moving looking friendly loyal</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b38a74866</td>\n",
       "      <td>adorable friendly</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1c92ce464</td>\n",
       "      <td>trying full clean able black safe under due on...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b10e7605a</td>\n",
       "      <td>subtle super lovable very big cute back advent...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1bf24d8be</td>\n",
       "      <td>friendly very</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>988988d5b</td>\n",
       "      <td>indoor happy good wan resident just</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cf7d3eec0</td>\n",
       "      <td>interested Calm thick daily soft large short g...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>85fc3c314</td>\n",
       "      <td>easy cute playful healthy</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>efbf1703a</td>\n",
       "      <td>Just</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7843a9dca</td>\n",
       "      <td>healthy skinny tall active</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>d341fbfd6</td>\n",
       "      <td>like old indoor neuter still lean just human</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1a76190c5</td>\n",
       "      <td>indoor over white handsome cute cross shaggy b...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>edb920079</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9415bc79e</td>\n",
       "      <td>gorgeous willing solid indoor looking big welc...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4c6fe4100</td>\n",
       "      <td>Open</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>eabb13cea</td>\n",
       "      <td>lovely</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>deff069a2</td>\n",
       "      <td>well looking friendly very</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ace21b6b1</td>\n",
       "      <td>interested then looking shy back old unable bo...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>ded648294</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>a99b34f95</td>\n",
       "      <td>forlorn fellow own looking wary very emaciated...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>f344595d7</td>\n",
       "      <td>quiet alert chance new ready lovely very healt...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>71b8ca636</td>\n",
       "      <td>back</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>f410b0841</td>\n",
       "      <td>mild old numerous perfect free Brown collar Fr...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>01ba86830</td>\n",
       "      <td>soft Curly due like</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>b8c54bc91</td>\n",
       "      <td>quiet docile</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>29ffe21fe</td>\n",
       "      <td>healthy active good old</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2824</th>\n",
       "      <td>3d985e1bf</td>\n",
       "      <td>male</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>fc7f7797c</td>\n",
       "      <td>lively endless good flat</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>5aff9022b</td>\n",
       "      <td>interested kindly</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827</th>\n",
       "      <td>09bc86c05</td>\n",
       "      <td>near temporary poor able female black still fo...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>bca099371</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>a0b82db0e</td>\n",
       "      <td>well permanent very playful under</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>e72a77ee4</td>\n",
       "      <td>poor like likely own cute back</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>350208e1a</td>\n",
       "      <td>quiet then little great looking good very sweet</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>83d15cf4d</td>\n",
       "      <td>smart still old healthy very potential perfect...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>34c4e74a9</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>8efcfe9c6</td>\n",
       "      <td>healthy important needy preventive basic able ...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>09a024035</td>\n",
       "      <td>interested</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>75bfbc825</td>\n",
       "      <td>male then even choice sure intelligent like ex...</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>a3dfe1991</td>\n",
       "      <td>interested</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>3ef9ac2a6</td>\n",
       "      <td>then over healthy just active ready</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>7db48c47a</td>\n",
       "      <td>cute sad own collar</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>bcee746ce</td>\n",
       "      <td>female front active very</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>e09807e6e</td>\n",
       "      <td>sweet good chance</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>d3faf6a0e</td>\n",
       "      <td>small</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>3b3b37546</td>\n",
       "      <td>interested busy active</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2844</th>\n",
       "      <td>98ae7d88d</td>\n",
       "      <td>Free</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>f5dc70d35</td>\n",
       "      <td>healthy</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14991 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PetID                                        Description  \\\n",
       "0     6296e909a                          alone near just temporary   \n",
       "1     3422e4906  near adorable pregnant acceptable long precaut...   \n",
       "2     5842f1ff5      interested alert Good good very master active   \n",
       "3     850a43f90      such like looking cute playful young handsome   \n",
       "4     d24c30b4b                                              stray   \n",
       "5     1caa6fcdb                      interested just outside still   \n",
       "6     97aa9eeac                                               just   \n",
       "7     c06d167ca                                healthy active sure   \n",
       "8     8b693ca84                                            serious   \n",
       "9     aaedd873d                           neuter happy active very   \n",
       "10    4a9793dfb                       long cute stray evident good   \n",
       "11    c02be41e6                                        healthy red   \n",
       "12    1fd342e17     fun-loving great moving looking friendly loyal   \n",
       "13    b38a74866                                  adorable friendly   \n",
       "14    1c92ce464  trying full clean able black safe under due on...   \n",
       "15    b10e7605a  subtle super lovable very big cute back advent...   \n",
       "16    1bf24d8be                                      friendly very   \n",
       "17    988988d5b                indoor happy good wan resident just   \n",
       "18    cf7d3eec0  interested Calm thick daily soft large short g...   \n",
       "19    85fc3c314                          easy cute playful healthy   \n",
       "20    efbf1703a                                               Just   \n",
       "21    7843a9dca                         healthy skinny tall active   \n",
       "22    d341fbfd6       like old indoor neuter still lean just human   \n",
       "23    1a76190c5  indoor over white handsome cute cross shaggy b...   \n",
       "24    edb920079                                                      \n",
       "25    9415bc79e  gorgeous willing solid indoor looking big welc...   \n",
       "26    4c6fe4100                                               Open   \n",
       "27    eabb13cea                                             lovely   \n",
       "28    deff069a2                         well looking friendly very   \n",
       "29    ace21b6b1  interested then looking shy back old unable bo...   \n",
       "...         ...                                                ...   \n",
       "2816  ded648294                                                      \n",
       "2817  a99b34f95  forlorn fellow own looking wary very emaciated...   \n",
       "2818  f344595d7  quiet alert chance new ready lovely very healt...   \n",
       "2819  71b8ca636                                               back   \n",
       "2820  f410b0841  mild old numerous perfect free Brown collar Fr...   \n",
       "2821  01ba86830                                soft Curly due like   \n",
       "2822  b8c54bc91                                       quiet docile   \n",
       "2823  29ffe21fe                            healthy active good old   \n",
       "2824  3d985e1bf                                               male   \n",
       "2825  fc7f7797c                           lively endless good flat   \n",
       "2826  5aff9022b                                  interested kindly   \n",
       "2827  09bc86c05  near temporary poor able female black still fo...   \n",
       "2828  bca099371                                                      \n",
       "2829  a0b82db0e                  well permanent very playful under   \n",
       "2830  e72a77ee4                     poor like likely own cute back   \n",
       "2831  350208e1a    quiet then little great looking good very sweet   \n",
       "2832  83d15cf4d  smart still old healthy very potential perfect...   \n",
       "2833  34c4e74a9                                                      \n",
       "2834  8efcfe9c6  healthy important needy preventive basic able ...   \n",
       "2835  09a024035                                         interested   \n",
       "2836  75bfbc825  male then even choice sure intelligent like ex...   \n",
       "2837  a3dfe1991                                         interested   \n",
       "2838  3ef9ac2a6                then over healthy just active ready   \n",
       "2839  7db48c47a                                cute sad own collar   \n",
       "2840  bcee746ce                           female front active very   \n",
       "2841  e09807e6e                                  sweet good chance   \n",
       "2842  d3faf6a0e                                              small   \n",
       "2843  3b3b37546                             interested busy active   \n",
       "2844  98ae7d88d                                               Free   \n",
       "2845  f5dc70d35                                            healthy   \n",
       "\n",
       "      Adjective_count  \n",
       "0                  25  \n",
       "1                  80  \n",
       "2                  45  \n",
       "3                  45  \n",
       "4                   5  \n",
       "5                  29  \n",
       "6                   4  \n",
       "7                  19  \n",
       "8                   7  \n",
       "9                  24  \n",
       "10                 28  \n",
       "11                 11  \n",
       "12                 46  \n",
       "13                 17  \n",
       "14                 54  \n",
       "15                 72  \n",
       "16                 13  \n",
       "17                 35  \n",
       "18                 54  \n",
       "19                 25  \n",
       "20                  4  \n",
       "21                 26  \n",
       "22                 44  \n",
       "23                 50  \n",
       "24                  0  \n",
       "25                 54  \n",
       "26                  4  \n",
       "27                  6  \n",
       "28                 26  \n",
       "29                 70  \n",
       "...               ...  \n",
       "2816                0  \n",
       "2817               57  \n",
       "2818               56  \n",
       "2819                4  \n",
       "2820              125  \n",
       "2821               19  \n",
       "2822               12  \n",
       "2823               23  \n",
       "2824                4  \n",
       "2825               24  \n",
       "2826               17  \n",
       "2827               74  \n",
       "2828                0  \n",
       "2829               33  \n",
       "2830               30  \n",
       "2831               47  \n",
       "2832              174  \n",
       "2833                0  \n",
       "2834              127  \n",
       "2835               10  \n",
       "2836              106  \n",
       "2837               10  \n",
       "2838               35  \n",
       "2839               19  \n",
       "2840               24  \n",
       "2841               17  \n",
       "2842                5  \n",
       "2843               22  \n",
       "2844                4  \n",
       "2845                7  \n",
       "\n",
       "[14991 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增加新的特征\n",
    "1、是否需要收费\n",
    "2、年份 floor(x/12)\n",
    "3、该品种的数量\n",
    "4、是否是稀有品种(分普通稀有和特别稀有)\n",
    "5、是否很常见\n",
    "6、Color的笛卡尔积（效果不好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train['IsFree'] = x_train['Fee'].apply(lambda x:1 if x>0 else 0)\n",
    "x_test['IsFree']  = x_test['Fee'].apply(lambda x:1 if x>0 else 0)\n",
    "\n",
    "x_train['Year'] = x_train['Age'].apply(lambda x:math.floor(x/12))\n",
    "x_test['Year']  = x_test['Age'].apply(lambda x:math.floor(x/12))\n",
    "\n",
    "x = x_train.append(x_test)\n",
    "x['Age_qcut'] = pd.qcut(x['Age'], 5,  duplicates='drop')\n",
    "x['Age_qcut'] = pd.factorize(x['Age_qcut'])[0]\n",
    "x_train = x_train.merge(x[['PetID','Age_qcut']], how='left', on='PetID')\n",
    "x_test  = x_test.merge(x[['PetID','Age_qcut']], how='left', on='PetID')\n",
    "\n",
    "x = x_train.append(x_test)\n",
    "Breed1_count = x.groupby('Breed1').size().to_frame('Breed1_count').reset_index()\n",
    "x_train = x_train.merge(Breed1_count, how='left', on='Breed1')\n",
    "x_test = x_test.merge(Breed1_count, how='left', on='Breed1')\n",
    "\n",
    "\n",
    "a = x['Breed1'].value_counts().sort_values(ascending = False).cumsum()/len(x)\n",
    "rare1_index = a[a > 0.85].index.tolist()\n",
    "x_train['IsRare1'] = x_train['Breed1'].isin(rare1_index).apply(lambda x:1 if x == True else 0)\n",
    "x_test['IsRare1']  = x_test['Breed1'].isin(rare1_index).apply(lambda x:1 if x == True else 0)\n",
    "rare2_index = a[a > 0.72].index.tolist()\n",
    "x_train['IsRare2'] = x_train['Breed1'].isin(rare2_index).apply(lambda x:1 if x == True else 0)\n",
    "x_test['IsRare2']  = x_test['Breed1'].isin(rare2_index).apply(lambda x:1 if x == True else 0)\n",
    "\n",
    "x_train['Is_Mixed Breed_ID307']          = x_train['Breed1'].apply(lambda x:1 if x == 307 else 0)\n",
    "x_test['Is_Mixed Breed_ID307']           = x_test['Breed1'].apply(lambda x:1 if x == 307 else 0)\n",
    "x_train['Is_Domestic Short Hair_ID266']  = x_train['Breed1'].apply(lambda x:1 if x == 266 else 0)\n",
    "x_test['Is_Domestic Short Hair_ID266']   = x_test['Breed1'].apply(lambda x:1 if x == 266 else 0)\n",
    "x_train['Is_Domestic Medium Hair_ID265'] = x_train['Breed1'].apply(lambda x:1 if x == 265 else 0)\n",
    "x_test['Is_Domestic Medium Hair_ID265']  = x_test['Breed1'].apply(lambda x:1 if x == 265 else 0)\n",
    "\n",
    "x_train['Is_COMMON'] = x_train['Breed1'].apply(lambda x:1 if (x == 265 or x == 307 or x == 266) else 0)\n",
    "x_test['Is_COMMON']  = x_test['Breed1'].apply(lambda x:1 if (x == 265 or x == 307 or x == 266) else 0)\n",
    "\n",
    "\n",
    "#是否是纯种\n",
    "\n",
    "\n",
    "#效果不好\n",
    "#x_train['Color_Mix'] = x_train['Color1'].astype(str)+x_train['Color2'].astype(str)+x_train['Color3'].astype(str)\n",
    "#x_train['Color_Mix'] = pd.factorize(x_train['Color_Mix'])[0]\n",
    "#x_test['Color_Mix'] = x_test['Color1'].astype(str)+x_test['Color2'].astype(str)+x_test['Color3'].astype(str)\n",
    "#x_test['Color_Mix'] = pd.factorize(x_test['Color_Mix'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Name', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n",
       "       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n",
       "       'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'RescuerID',\n",
       "       'VideoAmt', 'Description', 'PetID', 'PhotoAmt', 'sentence_magnitude',\n",
       "       'sentence_score', 'document_magnitude', 'document_score',\n",
       "       'SVD_describe_0', 'SVD_describe_1', 'SVD_describe_2', 'SVD_describe_3',\n",
       "       'SVD_describe_4', 'SVD_describe_5', 'SVD_describe_6', 'SVD_describe_7',\n",
       "       'SVD_describe_8', 'SVD_describe_9', 'IsFree', 'Year', 'Age_qcut',\n",
       "       'Breed1_count', 'IsRare1', 'IsRare2', 'Is_Mixed Breed_ID307',\n",
       "       'Is_Domestic Short Hair_ID266', 'Is_Domestic Medium Hair_ID265',\n",
       "       'Is_COMMON'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     2451\n",
       "2.0     2003\n",
       "3.0     1988\n",
       "5.0     1672\n",
       "4.0     1522\n",
       "6.0      484\n",
       "7.0      349\n",
       "0.0      271\n",
       "8.0      244\n",
       "9.0      176\n",
       "11.0     145\n",
       "10.0     145\n",
       "12.0      82\n",
       "13.0      73\n",
       "14.0      62\n",
       "15.0      40\n",
       "16.0      33\n",
       "20.0      24\n",
       "17.0      23\n",
       "19.0      17\n",
       "18.0      16\n",
       "30.0      15\n",
       "24.0      14\n",
       "21.0      14\n",
       "23.0      11\n",
       "22.0       8\n",
       "26.0       8\n",
       "28.0       7\n",
       "27.0       6\n",
       "25.0       6\n",
       "29.0       5\n",
       "Name: PhotoAmt, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['PhotoAmt'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RescuerID 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% RescuerID 处理\n",
    "\n",
    "df = df_train.append(df_test)\n",
    "data_rescuer = df.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "data_rescuer.columns = ['RescuerID', 'RescuerID_count']\n",
    "#data_rescuer['rank_Rescuer_count'] = data_rescuer['RescuerID_count'].rank(pct=True)\n",
    "\n",
    "x_train = x_train.merge(data_rescuer, how='left', on='RescuerID')\n",
    "x_test  = x_test.merge(data_rescuer, how='left', on='RescuerID')\n",
    "\n",
    "#x_train.drop(['RescuerID_count'], axis=1, inplace=True)\n",
    "#x_test.drop(['RescuerID_count'], axis=1, inplace=True)\n",
    "\n",
    "x_train['single'] = x_train['RescuerID_count'].apply(lambda x:1 if x<3 else 0)\n",
    "x_train['middle'] = x_train['RescuerID_count'].apply(lambda x:1 if (x>2 and x<6) else 0)\n",
    "x_train['Charities'] = x_train['RescuerID_count'].apply(lambda x:1 if x>5 else 0)\n",
    "\n",
    "x_test['single'] = x_test['RescuerID_count'].apply(lambda x:1 if x<3 else 0)\n",
    "x_test['middle'] = x_test['RescuerID_count'].apply(lambda x:1 if (x>2 and x<6) else 0)\n",
    "x_test['Charities'] = x_test['RescuerID_count'].apply(lambda x:1 if x>5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理Breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11914, 56) (3075, 56)\n"
     ]
    }
   ],
   "source": [
    "# 增加特征 是否有第二血统\n",
    "x_train['HasSecondBreed'] = x_train['Breed2'].map(lambda x:1 if x != 0 else 0)\n",
    "x_test['HasSecondBreed'] = x_test['Breed2'].map(lambda x:1 if x != 0 else 0)\n",
    "\n",
    "train_breed_main = x_train[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "\n",
    "train_breed_main = train_breed_main.iloc[:, 2:]\n",
    "train_breed_main = train_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "train_breed_second = x_train[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "\n",
    "train_breed_second = train_breed_second.iloc[:, 2:]\n",
    "train_breed_second = train_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "x_train = pd.concat(\n",
    "    [x_train, train_breed_main, train_breed_second], axis=1)\n",
    "\n",
    "##############\n",
    "test_breed_main = x_test[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "\n",
    "test_breed_main = test_breed_main.iloc[:, 2:]\n",
    "test_breed_main = test_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "test_breed_second = x_test[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "test_breed_second = test_breed_second.iloc[:, 2:]\n",
    "test_breed_second = test_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "x_test = pd.concat(\n",
    "    [x_test, test_breed_main, test_breed_second], axis=1)\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "categorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n",
    "#for i in categorical_columns:\n",
    "#    x_train.loc[:, i] = pd.factorize(x_train.loc[:, i])[0]\n",
    "#    x_test.loc[:,i]   = pd.factorize(x_test.loc[:, i])[0]\n",
    "\n",
    "# 增加特征 是否纯种\n",
    "x_train['True_Pure'] = 0\n",
    "x_train.loc[(x_train['main_breed_BreedName'] != 'Mixed Breed')&\n",
    "                    ((x_train['main_breed_BreedName'] == x_train['second_breed_BreedName'])|\n",
    "                   (x_train['second_breed_BreedName'].isnull())),'True_Pure'] = 1\n",
    "\n",
    "x_test['True_Pure'] = 0\n",
    "x_test.loc[(x_test['main_breed_BreedName'] != 'Mixed Breed')&\n",
    "                    ((x_test['main_breed_BreedName'] == x_test['second_breed_BreedName'])|\n",
    "                   (x_test['second_breed_BreedName'].isnull())),'True_Pure'] = 1\n",
    "\n",
    "x_train[['main_breed_BreedName', 'True_Pure', 'second_breed_BreedName']]\n",
    "\n",
    "#删除没用特征\n",
    "x_train.drop(['main_breed_BreedName', 'second_breed_BreedName', 'main_breed_Type', 'second_breed_Type'], axis=1, inplace=True)\n",
    "x_test.drop(['main_breed_BreedName', 'second_breed_BreedName', 'main_breed_Type', 'second_breed_Type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Name', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n",
       "       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n",
       "       'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'RescuerID',\n",
       "       'VideoAmt', 'Description', 'PetID', 'PhotoAmt', 'sentence_magnitude',\n",
       "       'sentence_score', 'document_magnitude', 'document_score',\n",
       "       'SVD_describe_0', 'SVD_describe_1', 'SVD_describe_2', 'SVD_describe_3',\n",
       "       'SVD_describe_4', 'SVD_describe_5', 'SVD_describe_6', 'SVD_describe_7',\n",
       "       'SVD_describe_8', 'SVD_describe_9', 'IsFree', 'Year', 'Age_qcut',\n",
       "       'Breed1_count', 'IsRare1', 'IsRare2', 'Is_Mixed Breed_ID307',\n",
       "       'Is_Domestic Short Hair_ID266', 'Is_Domestic Medium Hair_ID265',\n",
       "       'Is_COMMON', 'RescuerID_count', 'single', 'middle', 'Charities',\n",
       "       'HasSecondBreed', 'True_Pure'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 增加特征 是否纯种\n",
    "x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 等某些特征进行rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 对某些特征进行rank\n",
    "#cols_rank = ['sentence_magnitude', 'sentence_score', 'document_magnitude','document_score']#,\n",
    "#       'annots_score_MEAN', 'annots_score_SUM','color_score_MEAN', 'color_score_SUM', 'color_pixelfrac_MEAN',\n",
    "#       'color_pixelfrac_SUM', 'crop_conf_MEAN', 'crop_conf_SUM','crop_importance_MEAN', 'crop_importance_SUM']\n",
    "\n",
    "#x = x_train.append(x_test)\n",
    "#x[cols_rank] = x[cols_rank].fillna(0)\n",
    "#df_cols_rank = x[cols_rank].rank(pct=True).rename(columns=lambda s:'rank.'+s)\n",
    "#df_cols_rank = pd.concat([df_cols_rank, x['PetID']], axis=1)\n",
    "\n",
    "#x_train = x_train.merge(df_cols_rank, how='left', on='PetID')\n",
    "#x_test =  x_test.merge(df_cols_rank, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Name', 'RescuerID', 'Description', 'PetID', 'token', 'annots_top_desc']\n",
    "drop_columns = ['Name', 'RescuerID', 'Description', 'PetID']\n",
    "\n",
    "\n",
    "x_train.drop(drop_columns, axis=1, inplace=True)\n",
    "x_test.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "x_train = x_train.fillna(0)\n",
    "x_test  = x_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMRegressor\n",
    "\n",
    "model_lgb = LGBMRegressor(\n",
    "        learning_rate    = 0.1,\n",
    "        n_estimators     = 300,\n",
    "        max_depth        = 4,\n",
    "        num_leaves       = 8 ,\n",
    "        subsample        = 0.7,      #训练时采样一定比例的数据\t\n",
    "        colsample_bytree = 0.7,\n",
    "        n_jobs           = -1,\n",
    "        random_state     = 4,\n",
    "        objective        = 'regression',\n",
    "        eval_metric      = 'scorer',\n",
    "        min_child_samples = 3         #叶子节点具有的最小记录数\t\n",
    "\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10折后的Kappa加权得分为:带补充\n",
      "basinhopping step 0: f -0.424523\n",
      "basinhopping step 1: f -0.424697 trial_f -0.424697 accepted 1  lowest_f -0.424697\n",
      "found new global minimum on step 1 with function value -0.424697\n",
      "basinhopping step 2: f -0.423883 trial_f -0.423883 accepted 1  lowest_f -0.424697\n",
      "basinhopping step 3: f -0.425726 trial_f -0.425726 accepted 1  lowest_f -0.425726\n",
      "found new global minimum on step 3 with function value -0.425726\n",
      "basinhopping step 4: f -0.42595 trial_f -0.42595 accepted 1  lowest_f -0.42595\n",
      "found new global minimum on step 4 with function value -0.42595\n",
      "basinhopping step 5: f -0.423497 trial_f -0.423497 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 6: f -0.425468 trial_f -0.425468 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 7: f -0.425926 trial_f -0.425926 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 8: f -0.425862 trial_f -0.425862 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 9: f -0.42566 trial_f -0.42566 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 10: f -0.424862 trial_f -0.424862 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 11: f -0.423909 trial_f -0.423909 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 12: f -0.424428 trial_f -0.424428 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 13: f -0.424343 trial_f -0.424343 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 14: f -0.424828 trial_f -0.424828 accepted 1  lowest_f -0.42595\n",
      "basinhopping step 15: f -0.425385 trial_f -0.425385 accepted 1  lowest_f -0.42595\n",
      "lgb的最佳系数为[1.63050566 2.13466441 2.47428175 2.81409366]\n",
      "lgb后的分布: Counter({4.0: 858, 2.0: 834, 1.0: 680, 3.0: 665, 0.0: 38})\n",
      "融合后的二次加权Kappa系数为 0.39338008556589465\n",
      "y_test的真实分布为 Counter({4: 878, 2: 862, 1: 636, 3: 613, 0: 86})\n"
     ]
    }
   ],
   "source": [
    "y_lgb = split_score(model_lgb, x_train, y_train)\n",
    "\n",
    "coe = search_coef(y_lgb, y_train)\n",
    "best_lgb_coe = coe['x']\n",
    "print('lgb的最佳系数为{}'.format(best_lgb_coe))\n",
    "\n",
    "model_lgb.fit(x_train, y_train)\n",
    "result_lgb = model_lgb.predict(x_test)\n",
    "result_lgb_fix = fix_y(result_lgb, best_lgb_coe)\n",
    "print('lgb后的分布:',Counter(result_lgb_fix))\n",
    "print('融合后的二次加权Kappa系数为', metric(result_lgb_fix, y_test))\n",
    "print('y_test的真实分布为',Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11447,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RescuerID_count</th>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed1</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_9</th>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_1</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed1_count</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_8</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_0</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_4</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_7</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_5</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_score</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_score</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fee</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_magnitude</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_3</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_6</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_magnitude</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color1</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_2</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed2</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaturitySize</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FurLength</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sterilized</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vaccinated</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color2</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age_qcut</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dewormed</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middle</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoAmt</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color3</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charities</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_Pure</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Domestic Short Hair_ID266</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_COMMON</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasSecondBreed</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Domestic Medium Hair_ID265</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsRare2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Mixed Breed_ID307</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsFree</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsRare1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0\n",
       "RescuerID_count                180\n",
       "Breed1                         133\n",
       "Age                             98\n",
       "PhotoAmt                        98\n",
       "SVD_describe_9                  96\n",
       "SVD_describe_1                  93\n",
       "Breed1_count                    76\n",
       "SVD_describe_8                  76\n",
       "SVD_describe_0                  75\n",
       "SVD_describe_4                  70\n",
       "State                           69\n",
       "SVD_describe_7                  65\n",
       "SVD_describe_5                  65\n",
       "sentence_score                  65\n",
       "document_score                  59\n",
       "Fee                             58\n",
       "sentence_magnitude              56\n",
       "SVD_describe_3                  55\n",
       "SVD_describe_6                  52\n",
       "Quantity                        49\n",
       "document_magnitude              48\n",
       "Color1                          48\n",
       "SVD_describe_2                  47\n",
       "Breed2                          46\n",
       "MaturitySize                    41\n",
       "FurLength                       36\n",
       "Sterilized                      34\n",
       "Gender                          21\n",
       "Vaccinated                      21\n",
       "Color2                          20\n",
       "Age_qcut                        20\n",
       "Health                          20\n",
       "Dewormed                        19\n",
       "Year                            18\n",
       "middle                          12\n",
       "VideoAmt                        12\n",
       "Color3                          11\n",
       "Charities                        8\n",
       "single                           8\n",
       "True_Pure                        4\n",
       "Is_Domestic Short Hair_ID266     3\n",
       "Is_COMMON                        3\n",
       "HasSecondBreed                   3\n",
       "Type                             3\n",
       "Is_Domestic Medium Hair_ID265    2\n",
       "IsRare2                          2\n",
       "Is_Mixed Breed_ID307             1\n",
       "IsFree                           1\n",
       "IsRare1                          0"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#参数重要性\n",
    "model_lgb.feature_importances_\n",
    "a = pd.DataFrame(model_lgb.feature_importances_, index=x_train.columns)\n",
    "a.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 568, in _fit_and_score\n    test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 605, in _score\n    return _multimetric_score(estimator, X_test, y_test, scorer)\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 635, in _multimetric_score\n    score = scorer(estimator, X_test, y_test)\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\", line 98, in __call__\n    **self._kwargs)\n  File \"<ipython-input-495-58e377756040>\", line 4, in metric\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\", line 354, in cohen_kappa_score\n    sample_weight=sample_weight)\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\", line 253, in confusion_matrix\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n  File \"/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\", line 81, in _check_targets\n    \"and {1} targets\".format(type_true, type_pred))\nValueError: Classification metrics can't handle a mix of multiclass and continuous targets\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-516-4f73546c81cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgsearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mgsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "#调参\n",
    "parameters = {\n",
    "            'reg_alpha': [0,0.01,0.1,1],\n",
    "            'reg_lambda': [0,0.01,0.1,1]\n",
    "            }\n",
    "\n",
    "gsearch = GridSearchCV(model_lgb, param_grid=parameters, scoring=scorer, cv=10, n_jobs=-1)\n",
    "gsearch.fit(x_train, y_train)\n",
    "gsearch.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2',\n",
       "       'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n",
       "       'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'VideoAmt',\n",
       "       'PhotoAmt', 'sentence_magnitude', 'sentence_score',\n",
       "       'document_magnitude', 'document_score', 'SVD_describe_0',\n",
       "       'SVD_describe_1', 'SVD_describe_2', 'SVD_describe_3', 'SVD_describe_4',\n",
       "       'SVD_describe_5', 'SVD_describe_6', 'SVD_describe_7', 'SVD_describe_8',\n",
       "       'SVD_describe_9', 'IsFree', 'Year', 'Age_qcut', 'Breed1_count',\n",
       "       'IsRare1', 'IsRare2', 'Is_Mixed Breed_ID307',\n",
       "       'Is_Domestic Short Hair_ID266', 'Is_Domestic Medium Hair_ID265',\n",
       "       'Is_COMMON', 'RescuerID_count', 'single', 'middle', 'Charities',\n",
       "       'HasSecondBreed', 'True_Pure'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "model_lgb_class = LGBMClassifier(\n",
    "        learning_rate    = 0.1,\n",
    "        n_estimators     = 500,\n",
    "        max_depth        = 4,\n",
    "        num_leaves       = 8 ,\n",
    "        subsample        = 0.7,      #训练时采样一定比例的数据\t\n",
    "        colsample_bytree = 0.7,\n",
    "        n_jobs           = -1,\n",
    "        random_state     = 4,\n",
    "        eval_metric      = 'scorer',\n",
    "        min_child_samples = 3         #叶子节点具有的最小记录数\t\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31808482442869823"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb_class.fit(x_train, y_train)\n",
    "result_lgb_class = model_lgb_class.predict(x_test)\n",
    "metric(result_lgb_class, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RescuerID_count</th>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_4</th>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_6</th>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_8</th>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_3</th>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_1</th>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_7</th>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_5</th>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_9</th>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_0</th>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_score</th>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_describe_2</th>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed1</th>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_magnitude</th>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed1_count</th>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_magnitude</th>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_score</th>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fee</th>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color1</th>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed2</th>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaturitySize</th>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color2</th>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sterilized</th>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FurLength</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vaccinated</th>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age_qcut</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color3</th>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dewormed</th>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_Pure</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoAmt</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middle</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charities</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Domestic Short Hair_ID266</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_COMMON</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Domestic Medium Hair_ID265</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasSecondBreed</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsRare2</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Mixed Breed_ID307</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsRare1</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsFree</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0\n",
       "RescuerID_count                1003\n",
       "SVD_describe_4                  997\n",
       "SVD_describe_6                  860\n",
       "SVD_describe_8                  839\n",
       "SVD_describe_3                  829\n",
       "SVD_describe_1                  772\n",
       "SVD_describe_7                  769\n",
       "SVD_describe_5                  750\n",
       "SVD_describe_9                  741\n",
       "Age                             738\n",
       "SVD_describe_0                  738\n",
       "sentence_score                  689\n",
       "SVD_describe_2                  685\n",
       "PhotoAmt                        668\n",
       "Breed1                          639\n",
       "document_magnitude              514\n",
       "Breed1_count                    514\n",
       "sentence_magnitude              468\n",
       "document_score                  429\n",
       "State                           358\n",
       "Fee                             358\n",
       "Color1                          306\n",
       "Breed2                          302\n",
       "Quantity                        287\n",
       "MaturitySize                    228\n",
       "Color2                          214\n",
       "Sterilized                      212\n",
       "FurLength                       200\n",
       "Gender                          189\n",
       "Vaccinated                      142\n",
       "Age_qcut                        136\n",
       "Color3                          133\n",
       "Dewormed                        106\n",
       "True_Pure                        98\n",
       "Health                           94\n",
       "VideoAmt                         93\n",
       "middle                           76\n",
       "Year                             70\n",
       "Type                             44\n",
       "Charities                        36\n",
       "Is_Domestic Short Hair_ID266     34\n",
       "Is_COMMON                        25\n",
       "Is_Domestic Medium Hair_ID265    24\n",
       "single                           24\n",
       "HasSecondBreed                   21\n",
       "IsRare2                          14\n",
       "Is_Mixed Breed_ID307             14\n",
       "IsRare1                          10\n",
       "IsFree                            7"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#参数重要性\n",
    "model_lgb_class.feature_importances_\n",
    "a = pd.DataFrame(model_lgb_class.feature_importances_, index=x_train.columns)\n",
    "a.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.6, early_stopping_rounds=20, gamma=0,\n",
       "       learning_rate=0.05, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=5, missing=None, n_estimatores=500,\n",
       "       n_estimators=100, n_jobs=1, nthread=-1, objective='reg:linear',\n",
       "       random_state=0, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "model_xgb = XGBRegressor(\n",
    "    learning_rate    = 0.05,\n",
    "    n_estimatores    = 500, \n",
    "    \n",
    "    early_stopping_rounds=20,  \n",
    "    \n",
    "    max_depth        = 4, \n",
    "    min_child_weight = 5,\n",
    "    \n",
    "    gamma            = 0,\n",
    "    \n",
    "    subsample        =  0.8,\n",
    "    colsample_bytree = 0.6,\n",
    "    \n",
    "    reg_alpha        = 1,\n",
    "    reg_lambda       = 0.1,\n",
    "    nthread      = -1)\n",
    "#    objective        = 'regression',    \n",
    "#    eval_metric      = 'scorer')      \n",
    "\n",
    "model_xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10折后的Kappa加权得分为:带补充\n",
      "basinhopping step 0: f -0.408559\n",
      "basinhopping step 1: f -0.411163 trial_f -0.411163 accepted 1  lowest_f -0.411163\n",
      "found new global minimum on step 1 with function value -0.411163\n",
      "basinhopping step 2: f -0.410285 trial_f -0.410285 accepted 1  lowest_f -0.411163\n",
      "basinhopping step 3: f -0.410037 trial_f -0.410037 accepted 1  lowest_f -0.411163\n",
      "basinhopping step 4: f -0.411028 trial_f -0.411028 accepted 1  lowest_f -0.411163\n",
      "basinhopping step 5: f -0.410433 trial_f -0.410433 accepted 1  lowest_f -0.411163\n",
      "basinhopping step 6: f -0.411793 trial_f -0.411793 accepted 1  lowest_f -0.411793\n",
      "found new global minimum on step 6 with function value -0.411793\n",
      "basinhopping step 7: f -0.411807 trial_f -0.411807 accepted 1  lowest_f -0.411807\n",
      "found new global minimum on step 7 with function value -0.411807\n",
      "basinhopping step 8: f -0.408987 trial_f -0.408987 accepted 1  lowest_f -0.411807\n",
      "basinhopping step 9: f -0.409127 trial_f -0.409127 accepted 1  lowest_f -0.411807\n",
      "basinhopping step 10: f -0.41151 trial_f -0.41151 accepted 1  lowest_f -0.411807\n",
      "basinhopping step 11: f -0.41169 trial_f -0.41169 accepted 1  lowest_f -0.411807\n",
      "basinhopping step 12: f -0.410143 trial_f -0.410143 accepted 1  lowest_f -0.411807\n",
      "basinhopping step 13: f -0.411353 trial_f -0.411353 accepted 1  lowest_f -0.411807\n",
      "basinhopping step 14: f -0.411674 trial_f -0.411674 accepted 1  lowest_f -0.411807\n",
      "basinhopping step 15: f -0.411731 trial_f -0.411731 accepted 1  lowest_f -0.411807\n",
      "basinhopping step 16: f -0.411882 trial_f -0.411882 accepted 1  lowest_f -0.411882\n",
      "found new global minimum on step 16 with function value -0.411882\n",
      "basinhopping step 17: f -0.412312 trial_f -0.412312 accepted 1  lowest_f -0.412312\n",
      "found new global minimum on step 17 with function value -0.412312\n",
      "basinhopping step 18: f -0.41111 trial_f -0.41111 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 19: f -0.411374 trial_f -0.411374 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 20: f -0.411015 trial_f -0.411015 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 21: f -0.411394 trial_f -0.411394 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 22: f -0.410958 trial_f -0.410958 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 23: f -0.411283 trial_f -0.411283 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 24: f -0.411065 trial_f -0.411065 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 25: f -0.4118 trial_f -0.4118 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 26: f -0.410937 trial_f -0.410937 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 27: f -0.411545 trial_f -0.411545 accepted 1  lowest_f -0.412312\n",
      "basinhopping step 28: f -0.410597 trial_f -0.410597 accepted 1  lowest_f -0.412312\n",
      "xgb的最佳系数为[1.88766323 2.2364382  2.44181623 2.87192754]\n",
      "xgb后的分布: Counter({3.0: 850, 1.0: 838, 4.0: 676, 2.0: 601, 0.0: 110})\n",
      "xgb后的二次加权Kappa系数为 0.39114680770852905\n",
      "y_test的真实分布为 Counter({4: 878, 2: 862, 1: 636, 3: 613, 0: 86})\n"
     ]
    }
   ],
   "source": [
    "y_xgb = split_score(model_xgb, x_train, y_train)\n",
    "\n",
    "coe = search_coef(y_xgb, y_train)\n",
    "best_xgb_coe = coe['x']\n",
    "print('xgb的最佳系数为{}'.format(best_xgb_coe))\n",
    "\n",
    "result_xgb = model_xgb.predict(x_test)\n",
    "result_xgb_fix = fix_y(result_xgb, best_xgb_coe)\n",
    "print('xgb后的分布:',Counter(result_xgb_fix))\n",
    "print('xgb后的二次加权Kappa系数为', metric(result_xgb_fix, y_test))\n",
    "print('y_test的真实分布为',Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RescuerID_count</th>\n",
       "      <td>0.054110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.048630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed1</th>\n",
       "      <td>0.037671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annots_score_MEAN</th>\n",
       "      <td>0.037671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annots_score_SUM</th>\n",
       "      <td>0.036986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed1_count</th>\n",
       "      <td>0.032877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_annots_top_desc_1</th>\n",
       "      <td>0.026712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_annots_top_desc_2</th>\n",
       "      <td>0.026027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>0.026027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>0.025342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sterilized</th>\n",
       "      <td>0.024658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_1</th>\n",
       "      <td>0.023288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_0</th>\n",
       "      <td>0.023288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_Description_3</th>\n",
       "      <td>0.023288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_pixelfrac_SUM</th>\n",
       "      <td>0.022603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_2</th>\n",
       "      <td>0.021918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_3</th>\n",
       "      <td>0.020548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_score_SUM</th>\n",
       "      <td>0.020548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_pixelfrac_MEAN</th>\n",
       "      <td>0.018493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color1</th>\n",
       "      <td>0.017808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_token_4</th>\n",
       "      <td>0.017808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed2</th>\n",
       "      <td>0.017808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_token_3</th>\n",
       "      <td>0.016438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_4</th>\n",
       "      <td>0.015068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FurLength</th>\n",
       "      <td>0.014384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_annots_top_desc_4</th>\n",
       "      <td>0.014384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fee</th>\n",
       "      <td>0.014384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_token_1</th>\n",
       "      <td>0.013699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age_qcut</th>\n",
       "      <td>0.013014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_token_2</th>\n",
       "      <td>0.012329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaturitySize</th>\n",
       "      <td>0.006164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>0.006164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>0.006164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crop_importance_MEAN</th>\n",
       "      <td>0.005479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_annots_top_desc_3</th>\n",
       "      <td>0.005479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_token_2</th>\n",
       "      <td>0.004795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crop_importance_SUM</th>\n",
       "      <td>0.004795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_magnitude</th>\n",
       "      <td>0.004795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Mixed Breed_ID307</th>\n",
       "      <td>0.003425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank.document_score</th>\n",
       "      <td>0.003425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_token_0</th>\n",
       "      <td>0.003425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color3</th>\n",
       "      <td>0.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_magnitude</th>\n",
       "      <td>0.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crop_conf_SUM</th>\n",
       "      <td>0.002055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank.sentence_magnitude</th>\n",
       "      <td>0.002055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_Pure</th>\n",
       "      <td>0.002055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charities</th>\n",
       "      <td>0.002055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsFree</th>\n",
       "      <td>0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middle</th>\n",
       "      <td>0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color2</th>\n",
       "      <td>0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crop_conf_MEAN</th>\n",
       "      <td>0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoAmt</th>\n",
       "      <td>0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_annots_top_desc_2</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasSecondBreed</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Domestic Medium Hair_ID265</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_Domestic Short Hair_ID266</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsRare1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0\n",
       "RescuerID_count                0.054110\n",
       "Age                            0.048630\n",
       "Breed1                         0.037671\n",
       "annots_score_MEAN              0.037671\n",
       "annots_score_SUM               0.036986\n",
       "Breed1_count                   0.032877\n",
       "SVD_annots_top_desc_1          0.026712\n",
       "SVD_annots_top_desc_2          0.026027\n",
       "Quantity                       0.026027\n",
       "State                          0.025342\n",
       "Sterilized                     0.024658\n",
       "SVD_Description_1              0.023288\n",
       "SVD_Description_0              0.023288\n",
       "NMF_Description_3              0.023288\n",
       "color_pixelfrac_SUM            0.022603\n",
       "SVD_Description_2              0.021918\n",
       "SVD_Description_3              0.020548\n",
       "color_score_SUM                0.020548\n",
       "color_pixelfrac_MEAN           0.018493\n",
       "Color1                         0.017808\n",
       "SVD_token_4                    0.017808\n",
       "Breed2                         0.017808\n",
       "SVD_token_3                    0.016438\n",
       "SVD_Description_4              0.015068\n",
       "FurLength                      0.014384\n",
       "SVD_annots_top_desc_4          0.014384\n",
       "Fee                            0.014384\n",
       "NMF_token_1                    0.013699\n",
       "Age_qcut                       0.013014\n",
       "SVD_token_2                    0.012329\n",
       "...                                 ...\n",
       "MaturitySize                   0.006164\n",
       "Year                           0.006164\n",
       "single                         0.006164\n",
       "crop_importance_MEAN           0.005479\n",
       "NMF_annots_top_desc_3          0.005479\n",
       "NMF_token_2                    0.004795\n",
       "crop_importance_SUM            0.004795\n",
       "document_magnitude             0.004795\n",
       "Is_Mixed Breed_ID307           0.003425\n",
       "rank.document_score            0.003425\n",
       "NMF_token_0                    0.003425\n",
       "Color3                         0.002740\n",
       "sentence_magnitude             0.002740\n",
       "crop_conf_SUM                  0.002055\n",
       "rank.sentence_magnitude        0.002055\n",
       "True_Pure                      0.002055\n",
       "Charities                      0.002055\n",
       "IsFree                         0.001370\n",
       "middle                         0.001370\n",
       "Color2                         0.001370\n",
       "Type                           0.001370\n",
       "crop_conf_MEAN                 0.001370\n",
       "VideoAmt                       0.000685\n",
       "Health                         0.000000\n",
       "NMF_annots_top_desc_2          0.000000\n",
       "PhotoAmt                       0.000000\n",
       "HasSecondBreed                 0.000000\n",
       "Is_Domestic Medium Hair_ID265  0.000000\n",
       "Is_Domestic Short Hair_ID266   0.000000\n",
       "IsRare1                        0.000000\n",
       "\n",
       "[83 rows x 1 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#参数重要性\n",
    "model_xgb.feature_importances_\n",
    "a = pd.DataFrame(model_xgb.feature_importances_, index=x_train.columns)\n",
    "a.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 12, 'min_child_weight': 11}"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#调参\n",
    "parameters = {'max_depth': range(6,14,2), 'min_child_weight': range(3,13,2)}\n",
    "gsearch = GridSearchCV(model_xgb, param_grid=parameters, scoring=scorer, cv=10, n_jobs=-1)\n",
    "gsearch.fit(x_train, y_train)\n",
    "gsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=0.7, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=3, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators      = 250,\n",
    "    \n",
    "    max_features      = 0.7,#选择最适属性时划分的特征不能超过此值。\n",
    "#    max_depth         = 6, #设置树的最大深度，默认为None\n",
    "    \n",
    "    min_samples_leaf  = 3, #叶子节点最少的样本数\n",
    "    min_samples_split = 2,#根据属性划分节点时，每个划分最少的样本数\n",
    "    \n",
    "    criterion = 'mse',\n",
    "    \n",
    "    n_jobs            = -1\n",
    "    )\n",
    "\n",
    "model_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-517-0d771592ccb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcoe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_coef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_rf_coe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lgb的最佳系数为{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_rf_coe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-496-7df61a7aacff>\u001b[0m in \u001b[0;36msplit_score\u001b[0;34m(model, x, y, n)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1337\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0my_pre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 333\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_rf = split_score(model_rf, x_train, y_train)\n",
    "\n",
    "coe = search_coef(y_rf, y_train)\n",
    "best_rf_coe = coe['x']\n",
    "print('lgb的最佳系数为{}'.format(best_rf_coe))\n",
    "    \n",
    "result_rf = model_rf.predict(x_test)\n",
    "result_rf_fix = fix_y(result_rf, best_rf_coe)\n",
    "print('随机森林融合后的分布:',Counter(result_rf_fix))\n",
    "print('融合后的二次加权Kappa系数为', metric(result_rf_fix, y_test))\n",
    "print('y_test的真实分布为',Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.097558</td>\n",
       "      <td>1.984937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.339441</td>\n",
       "      <td>2.366128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.230871</td>\n",
       "      <td>2.241388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.255839</td>\n",
       "      <td>3.019631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.142082</td>\n",
       "      <td>2.266630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.994521</td>\n",
       "      <td>2.046473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.106402</td>\n",
       "      <td>2.015116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.500434</td>\n",
       "      <td>2.497560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.855198</td>\n",
       "      <td>1.979238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.210996</td>\n",
       "      <td>3.091297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.518604</td>\n",
       "      <td>2.491677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.802365</td>\n",
       "      <td>1.848688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.527741</td>\n",
       "      <td>2.362114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.607212</td>\n",
       "      <td>3.570138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.180422</td>\n",
       "      <td>2.271126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.524528</td>\n",
       "      <td>2.538908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.157132</td>\n",
       "      <td>3.079412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.697916</td>\n",
       "      <td>2.609636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.029820</td>\n",
       "      <td>3.069695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.350896</td>\n",
       "      <td>3.214154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.999598</td>\n",
       "      <td>1.997315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.442628</td>\n",
       "      <td>2.306229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.244408</td>\n",
       "      <td>2.245876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.290397</td>\n",
       "      <td>2.379848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.442705</td>\n",
       "      <td>2.409524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.223835</td>\n",
       "      <td>2.162406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.210905</td>\n",
       "      <td>2.279722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.091902</td>\n",
       "      <td>2.152732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.079974</td>\n",
       "      <td>2.972386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.495290</td>\n",
       "      <td>2.582903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12979</th>\n",
       "      <td>2.274577</td>\n",
       "      <td>2.271704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12980</th>\n",
       "      <td>1.753220</td>\n",
       "      <td>1.819787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12981</th>\n",
       "      <td>2.524830</td>\n",
       "      <td>2.606370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12982</th>\n",
       "      <td>2.190788</td>\n",
       "      <td>2.237564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12983</th>\n",
       "      <td>2.569545</td>\n",
       "      <td>2.577725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12984</th>\n",
       "      <td>2.900182</td>\n",
       "      <td>2.763938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12985</th>\n",
       "      <td>3.449826</td>\n",
       "      <td>3.280276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12986</th>\n",
       "      <td>1.920994</td>\n",
       "      <td>1.956692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12987</th>\n",
       "      <td>2.651778</td>\n",
       "      <td>2.671248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12988</th>\n",
       "      <td>2.227537</td>\n",
       "      <td>2.244693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12989</th>\n",
       "      <td>2.680136</td>\n",
       "      <td>2.669464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12990</th>\n",
       "      <td>2.555987</td>\n",
       "      <td>2.519849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12991</th>\n",
       "      <td>2.480280</td>\n",
       "      <td>2.479470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12992</th>\n",
       "      <td>2.091111</td>\n",
       "      <td>2.083256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12993</th>\n",
       "      <td>2.057247</td>\n",
       "      <td>2.289635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12994</th>\n",
       "      <td>3.563160</td>\n",
       "      <td>3.459157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12995</th>\n",
       "      <td>2.901801</td>\n",
       "      <td>2.669726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12996</th>\n",
       "      <td>2.834868</td>\n",
       "      <td>2.807584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12997</th>\n",
       "      <td>3.434263</td>\n",
       "      <td>3.298696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12998</th>\n",
       "      <td>2.971140</td>\n",
       "      <td>2.922766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12999</th>\n",
       "      <td>2.260223</td>\n",
       "      <td>2.200520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>2.358882</td>\n",
       "      <td>2.204040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13001</th>\n",
       "      <td>3.815206</td>\n",
       "      <td>3.579027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13002</th>\n",
       "      <td>2.943039</td>\n",
       "      <td>2.629627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13003</th>\n",
       "      <td>1.858243</td>\n",
       "      <td>1.841810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13004</th>\n",
       "      <td>1.751459</td>\n",
       "      <td>1.904601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13005</th>\n",
       "      <td>3.043252</td>\n",
       "      <td>3.042950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13006</th>\n",
       "      <td>2.253299</td>\n",
       "      <td>2.211852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13007</th>\n",
       "      <td>2.883589</td>\n",
       "      <td>2.796529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13008</th>\n",
       "      <td>2.283264</td>\n",
       "      <td>2.300014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13009 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0      2.097558  1.984937\n",
       "1      2.339441  2.366128\n",
       "2      2.230871  2.241388\n",
       "3      3.255839  3.019631\n",
       "4      2.142082  2.266630\n",
       "5      1.994521  2.046473\n",
       "6      2.106402  2.015116\n",
       "7      2.500434  2.497560\n",
       "8      1.855198  1.979238\n",
       "9      3.210996  3.091297\n",
       "10     2.518604  2.491677\n",
       "11     1.802365  1.848688\n",
       "12     2.527741  2.362114\n",
       "13     3.607212  3.570138\n",
       "14     2.180422  2.271126\n",
       "15     2.524528  2.538908\n",
       "16     3.157132  3.079412\n",
       "17     2.697916  2.609636\n",
       "18     3.029820  3.069695\n",
       "19     3.350896  3.214154\n",
       "20     1.999598  1.997315\n",
       "21     2.442628  2.306229\n",
       "22     2.244408  2.245876\n",
       "23     2.290397  2.379848\n",
       "24     2.442705  2.409524\n",
       "25     2.223835  2.162406\n",
       "26     2.210905  2.279722\n",
       "27     2.091902  2.152732\n",
       "28     3.079974  2.972386\n",
       "29     2.495290  2.582903\n",
       "...         ...       ...\n",
       "12979  2.274577  2.271704\n",
       "12980  1.753220  1.819787\n",
       "12981  2.524830  2.606370\n",
       "12982  2.190788  2.237564\n",
       "12983  2.569545  2.577725\n",
       "12984  2.900182  2.763938\n",
       "12985  3.449826  3.280276\n",
       "12986  1.920994  1.956692\n",
       "12987  2.651778  2.671248\n",
       "12988  2.227537  2.244693\n",
       "12989  2.680136  2.669464\n",
       "12990  2.555987  2.519849\n",
       "12991  2.480280  2.479470\n",
       "12992  2.091111  2.083256\n",
       "12993  2.057247  2.289635\n",
       "12994  3.563160  3.459157\n",
       "12995  2.901801  2.669726\n",
       "12996  2.834868  2.807584\n",
       "12997  3.434263  3.298696\n",
       "12998  2.971140  2.922766\n",
       "12999  2.260223  2.200520\n",
       "13000  2.358882  2.204040\n",
       "13001  3.815206  3.579027\n",
       "13002  2.943039  2.629627\n",
       "13003  1.858243  1.841810\n",
       "13004  1.751459  1.904601\n",
       "13005  3.043252  3.042950\n",
       "13006  2.253299  2.211852\n",
       "13007  2.883589  2.796529\n",
       "13008  2.283264  2.300014\n",
       "\n",
       "[13009 rows x 2 columns]"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame([y_lgb, y_xgb])\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "融合后的二次加权Kappa系数为 0.4254420615705451\n"
     ]
    }
   ],
   "source": [
    "best_coe  = (best_lgb_coe + best_xgb_coe) / 2\n",
    "best_coe = best_xgb_coe\n",
    "\n",
    "result = (result_lgb + result_xgb) / 2\n",
    "result_fix = fix_y(result, best_coe)\n",
    "\n",
    "print('融合后的二次加权Kappa系数为', metric(result_fix, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lgb = model_lgb.predict(x_train)\n",
    "\n",
    "y_xgb = model_xgb.predict(x_train)\n",
    "y_rf  = model_rf.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "融合后的分布: Counter({2.0: 1140, 3.0: 840, 1.0: 194, 4.0: 143})\n",
      "融合后的二次加权Kappa系数为 0.30060168046082947\n",
      "y_test的真实分布为 Counter({2: 655, 4: 552, 3: 531, 1: 521, 0: 58})\n"
     ]
    }
   ],
   "source": [
    "y = (result_lgb + result_xgb + result_rf)/3\n",
    "result = fix_y(y, best_coe)\n",
    "\n",
    "print('融合后的分布:',Counter(result))\n",
    "print('融合后的二次加权Kappa系数为', metric(result, y_test))\n",
    "print('y_test的真实分布为',Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Breed1</th>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RescuerID_count</th>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annots_score_MEAN</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_breed_BreedName</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annots_score_SUM</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_9</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_8</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fee</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breed2</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_7</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_score_SUM</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_3</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_Description_7</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second_breed_BreedName</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_Description_8</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color1</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_6</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_1</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sterilized</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FurLength</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_pixelfrac_SUM</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_Description_9</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_score</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_token_3</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_2</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_token_7</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_token_6</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_token_8</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_Description_2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_token_0</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_token_8</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD_Description_0</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank.sentence_score</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_magnitude</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhotoAmt</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_Description_5</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dewormed</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VideoAmt</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_token_6</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_token_4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank.document_magnitude</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasSecondBreed</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_score</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF_Description_6</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank.document_score</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charities</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middle</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crop_conf_MEAN</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_breed_Type</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsFree</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second_breed_Type</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "Breed1                   111\n",
       "Age                       97\n",
       "RescuerID_count           96\n",
       "annots_score_MEAN         69\n",
       "State                     63\n",
       "main_breed_BreedName      56\n",
       "annots_score_SUM          49\n",
       "SVD_Description_9         43\n",
       "Quantity                  42\n",
       "SVD_Description_8         41\n",
       "Fee                       40\n",
       "Breed2                    40\n",
       "SVD_Description_7         37\n",
       "color_score_SUM           36\n",
       "SVD_Description_3         36\n",
       "NMF_Description_7         33\n",
       "second_breed_BreedName    31\n",
       "NMF_Description_8         30\n",
       "Color1                    30\n",
       "SVD_Description_6         30\n",
       "SVD_Description_1         27\n",
       "Sterilized                27\n",
       "FurLength                 27\n",
       "color_pixelfrac_SUM       27\n",
       "NMF_Description_9         24\n",
       "document_score            24\n",
       "NMF_token_3               24\n",
       "SVD_Description_2         22\n",
       "NMF_token_7               22\n",
       "SVD_token_6               21\n",
       "...                      ...\n",
       "NMF_token_8                9\n",
       "NMF_Description_2          9\n",
       "SVD_token_0                9\n",
       "SVD_token_8                8\n",
       "Year                       8\n",
       "SVD_Description_0          8\n",
       "Health                     7\n",
       "rank.sentence_score        7\n",
       "single                     7\n",
       "sentence_magnitude         7\n",
       "PhotoAmt                   7\n",
       "Type                       6\n",
       "NMF_Description_5          6\n",
       "Dewormed                   5\n",
       "VideoAmt                   5\n",
       "NMF_token_6                4\n",
       "NMF_token_4                4\n",
       "rank.document_magnitude    4\n",
       "Color3                     4\n",
       "HasSecondBreed             4\n",
       "sentence_score             4\n",
       "NMF_Description_6          4\n",
       "rank.document_score        4\n",
       "Charities                  2\n",
       "middle                     2\n",
       "crop_conf_MEAN             2\n",
       "main_breed_Type            1\n",
       "Color2                     1\n",
       "IsFree                     1\n",
       "second_breed_Type          0\n",
       "\n",
       "[89 rows x 1 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征重要性\n",
    "model_lgb.feature_importances_\n",
    "a = pd.DataFrame(model_lgb.feature_importances_, index=x_train.columns)\n",
    "a.sort_values(by=0, ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
